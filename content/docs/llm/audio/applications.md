---
title: "éŸ³é¢‘åº”ç”¨"
weight: 7
---

# éŸ³é¢‘åº”ç”¨å®æˆ˜

æœ¬èŠ‚ä»‹ç»å¦‚ä½•å°†éŸ³é¢‘æ¨¡å‹é›†æˆåˆ°å®é™…åº”ç”¨ä¸­ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–å’Œéƒ¨ç½²å®è·µã€‚

## å…¸å‹åº”ç”¨åœºæ™¯

### è¯­éŸ³åŠ©æ‰‹

```
ç”¨æˆ·è¯­éŸ³ â†’ [ASR] â†’ æ–‡æœ¬ â†’ [LLM] â†’ å›å¤æ–‡æœ¬ â†’ [TTS] â†’ è¯­éŸ³å›å¤

ç¤ºä¾‹: "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
      â†“ ASR
      "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
      â†“ LLM
      "ä»Šå¤©åŒ—äº¬æ™´å¤©ï¼Œæ°”æ¸© 25 åº¦ã€‚"
      â†“ TTS
      ğŸ”Š è¯­éŸ³æ’­æŠ¥
```

### è¯­éŸ³è½¬å†™æœåŠ¡

```
ä¼šè®®å½•éŸ³ â†’ [VAD] â†’ åˆ†æ®µéŸ³é¢‘ â†’ [ASR] â†’ æ–‡æœ¬ â†’ [æ ¼å¼åŒ–] â†’ ä¼šè®®çºªè¦

åŠŸèƒ½:
â€¢ è¯´è¯äººåˆ†ç¦»
â€¢ æ—¶é—´æˆ³æ ‡æ³¨
â€¢ å…³é”®è¯æå–
â€¢ æ‘˜è¦ç”Ÿæˆ
```

### æœ‰å£°ä¹¦/æ’­å®¢ç”Ÿæˆ

```
æ–‡æœ¬å†…å®¹ â†’ [åˆ†æ®µ] â†’ [TTS] â†’ éŸ³é¢‘ç‰‡æ®µ â†’ [åˆå¹¶] â†’ æœ€ç»ˆéŸ³é¢‘

åŠŸèƒ½:
â€¢ å¤šè¯´è¯äºº
â€¢ æƒ…æ„Ÿæ§åˆ¶
â€¢ èƒŒæ™¯éŸ³ä¹
```

## åº”ç”¨æ¶æ„è®¾è®¡

### ç®€å•æ¶æ„ï¼ˆå•æœºï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   åº”ç”¨æœåŠ¡                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   ASR    â”‚  â”‚   LLM    â”‚  â”‚   TTS    â”‚      â”‚
â”‚  â”‚  Model   â”‚  â”‚  Model   â”‚  â”‚  Model   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                     â†‘                           â”‚
â”‚               GPU (å…±äº«æ˜¾å­˜)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¾®æœåŠ¡æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â†’â”‚   Gateway   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                 â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ASR Service â”‚   â”‚ LLM Service â”‚   â”‚ TTS Service â”‚
â”‚  (GPU #1)   â”‚   â”‚  (GPU #2)   â”‚   â”‚  (GPU #3)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                 â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Message Queue                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æµå¼å¤„ç†æ¶æ„

```python
"""æµå¼è¯­éŸ³å¤„ç†æ¶æ„"""

import asyncio
from transformers import pipeline
import sounddevice as sd
import numpy as np

class StreamingAudioProcessor:
    def __init__(self):
        self.asr = pipeline("automatic-speech-recognition",
                           model="openai/whisper-small",
                           chunk_length_s=5)
        self.tts = pipeline("text-to-speech",
                           model="suno/bark-small")

    async def process_stream(self, audio_stream):
        """å¤„ç†éŸ³é¢‘æµ"""
        buffer = []

        async for chunk in audio_stream:
            buffer.append(chunk)

            # ç´¯ç§¯è¶³å¤Ÿæ•°æ®åå¤„ç†
            if len(buffer) >= 5:  # 5 ç§’
                audio = np.concatenate(buffer)
                buffer = []

                # ASR
                text = self.asr(audio)["text"]
                yield {"type": "transcription", "text": text}

    async def synthesize_stream(self, text_stream):
        """æµå¼ TTS"""
        async for text in text_stream:
            # åˆ†å¥å¤„ç†
            sentences = self._split_sentences(text)
            for sentence in sentences:
                audio = self.tts(sentence)
                yield audio["audio"]

    def _split_sentences(self, text):
        import re
        return re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', text)
```

## æ¨¡å‹é›†æˆ

### è¯­éŸ³åŠ©æ‰‹å®Œæ•´å®ç°

```python
"""å®Œæ•´è¯­éŸ³åŠ©æ‰‹å®ç°"""

from transformers import pipeline
import soundfile as sf
import numpy as np

class VoiceAssistant:
    def __init__(self,
                 asr_model="openai/whisper-small",
                 tts_model="microsoft/speecht5_tts"):
        """åˆå§‹åŒ–è¯­éŸ³åŠ©æ‰‹"""
        # ASR
        self.asr = pipeline(
            "automatic-speech-recognition",
            model=asr_model,
            device=0
        )

        # TTS
        self.tts = pipeline(
            "text-to-speech",
            model=tts_model,
            device=0
        )

        # è¯´è¯äººåµŒå…¥ (SpeechT5 éœ€è¦)
        if "speecht5" in tts_model:
            from datasets import load_dataset
            embeddings = load_dataset(
                "Matthijs/cmu-arctic-xvectors",
                split="validation"
            )
            self.speaker_emb = embeddings[7306]["xvector"]
        else:
            self.speaker_emb = None

    def listen(self, audio_path):
        """è¯­éŸ³è½¬æ–‡æœ¬"""
        result = self.asr(audio_path)
        return result["text"]

    def speak(self, text, output_path="response.wav"):
        """æ–‡æœ¬è½¬è¯­éŸ³"""
        if self.speaker_emb:
            speech = self.tts(
                text,
                forward_params={"speaker_embeddings": self.speaker_emb}
            )
        else:
            speech = self.tts(text)

        sf.write(output_path, speech["audio"],
                samplerate=speech["sampling_rate"])
        return output_path

    def process(self, audio_path, llm_callback):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        # 1. è¯­éŸ³è½¬æ–‡æœ¬
        user_text = self.listen(audio_path)
        print(f"ç”¨æˆ·: {user_text}")

        # 2. LLM ç”Ÿæˆå›å¤
        response_text = llm_callback(user_text)
        print(f"åŠ©æ‰‹: {response_text}")

        # 3. æ–‡æœ¬è½¬è¯­éŸ³
        audio_path = self.speak(response_text)
        return audio_path

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    assistant = VoiceAssistant()

    # æ¨¡æ‹Ÿ LLM å›è°ƒ
    def simple_llm(text):
        if "å¤©æ°”" in text:
            return "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé€‚åˆå‡ºé—¨ã€‚"
        return "æŠ±æ­‰ï¼Œæˆ‘ä¸å¤ªç†è§£ä½ çš„é—®é¢˜ã€‚"

    # å¤„ç†
    result = assistant.process("user_audio.wav", simple_llm)
    print(f"å›å¤éŸ³é¢‘: {result}")
```

### ä¼šè®®è½¬å†™ç³»ç»Ÿ

```python
"""ä¼šè®®è½¬å†™ç³»ç»Ÿ"""

from transformers import pipeline
from datasets import Audio
import numpy as np

class MeetingTranscriber:
    def __init__(self):
        # ASR with timestamps
        self.asr = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-large-v3",
            return_timestamps=True,
            chunk_length_s=30,
            device=0
        )

    def transcribe(self, audio_path):
        """è½¬å†™ä¼šè®®éŸ³é¢‘"""
        result = self.asr(audio_path)
        return self._format_transcript(result)

    def _format_transcript(self, result):
        """æ ¼å¼åŒ–è½¬å†™ç»“æœ"""
        transcript = []

        for chunk in result.get("chunks", []):
            start, end = chunk["timestamp"]
            text = chunk["text"]
            transcript.append({
                "start": start,
                "end": end,
                "text": text.strip()
            })

        return transcript

    def export_srt(self, transcript, output_path):
        """å¯¼å‡º SRT å­—å¹•"""
        def format_time(seconds):
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            millis = int((seconds - int(seconds)) * 1000)
            return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

        with open(output_path, "w", encoding="utf-8") as f:
            for i, item in enumerate(transcript, 1):
                f.write(f"{i}\n")
                f.write(f"{format_time(item['start'])} --> {format_time(item['end'])}\n")
                f.write(f"{item['text']}\n\n")

    def export_txt(self, transcript, output_path):
        """å¯¼å‡ºçº¯æ–‡æœ¬"""
        with open(output_path, "w", encoding="utf-8") as f:
            for item in transcript:
                f.write(f"[{item['start']:.1f}s] {item['text']}\n")

# ä½¿ç”¨ç¤ºä¾‹
transcriber = MeetingTranscriber()
transcript = transcriber.transcribe("meeting.wav")
transcriber.export_srt(transcript, "meeting.srt")
```

### æœ‰å£°ä¹¦ç”Ÿæˆå™¨

```python
"""æœ‰å£°ä¹¦ç”Ÿæˆå™¨"""

from transformers import pipeline
import soundfile as sf
import numpy as np
import re

class AudiobookGenerator:
    def __init__(self, model="suno/bark-small"):
        self.tts = pipeline("text-to-speech", model=model, device=0)

    def generate(self, text, output_path, max_chunk=200):
        """ç”Ÿæˆæœ‰å£°ä¹¦"""
        # åˆ†æ®µ
        chunks = self._split_text(text, max_chunk)

        # é€æ®µåˆæˆ
        audio_parts = []
        for i, chunk in enumerate(chunks):
            print(f"åˆæˆè¿›åº¦: {i+1}/{len(chunks)}")
            speech = self.tts(chunk)
            audio_parts.append(speech["audio"])

            # æ·»åŠ çŸ­æš‚åœé¡¿
            pause = np.zeros(int(speech["sampling_rate"] * 0.5))
            audio_parts.append(pause)

        # åˆå¹¶
        final_audio = np.concatenate(audio_parts)
        sf.write(output_path, final_audio, samplerate=speech["sampling_rate"])
        return output_path

    def _split_text(self, text, max_length):
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split("\n\n")
        chunks = []

        for para in paragraphs:
            if len(para) <= max_length:
                chunks.append(para)
            else:
                # æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s*', para)
                current = ""
                for s in sentences:
                    if len(current) + len(s) <= max_length:
                        current += s + " "
                    else:
                        if current:
                            chunks.append(current.strip())
                        current = s + " "
                if current:
                    chunks.append(current.strip())

        return chunks

# ä½¿ç”¨ç¤ºä¾‹
generator = AudiobookGenerator()
book_text = """
ç¬¬ä¸€ç« 

ä»å‰æœ‰ä¸€åº§å±±ï¼Œå±±ä¸Šæœ‰ä¸€åº§åº™ã€‚åº™é‡Œæœ‰ä¸ªè€å’Œå°šå’Œä¸€ä¸ªå°å’Œå°šã€‚

æœ‰ä¸€å¤©ï¼Œè€å’Œå°šå¯¹å°å’Œå°šè¯´ï¼š"æˆ‘ç»™ä½ è®²ä¸ªæ•…äº‹å§ã€‚"

å°å’Œå°šé«˜å…´åœ°è¯´ï¼š"å¥½å•Šå¥½å•Šï¼"
"""
generator.generate(book_text, "audiobook.wav")
```

## æ€§èƒ½ä¼˜åŒ–

### GPU æ˜¾å­˜ä¼˜åŒ–

```python
import torch
from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig

# 1. ä½¿ç”¨åŠç²¾åº¦
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v3",
    torch_dtype=torch.float16
)

# 2. 4-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v3",
    quantization_config=bnb_config
)

# 3. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒæ—¶ï¼‰
model.gradient_checkpointing_enable()
```

### æ¨ç†åŠ é€Ÿ

```python
# 1. ä½¿ç”¨ Flash Attention
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v3",
    attn_implementation="flash_attention_2"
)

# 2. ä½¿ç”¨ BetterTransformer
model = model.to_bettertransformer()

# 3. æ‰¹å¤„ç†
asr = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small",
    batch_size=8
)
results = asr(["audio1.wav", "audio2.wav", "audio3.wav"])
```

### å¹¶å‘å¤„ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncAudioProcessor:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.asr = pipeline("automatic-speech-recognition",
                           model="openai/whisper-small")

    async def process_batch(self, audio_files):
        """å¹¶å‘å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"""
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.executor, self.asr, f)
            for f in audio_files
        ]
        results = await asyncio.gather(*tasks)
        return results

# ä½¿ç”¨
async def main():
    processor = AsyncAudioProcessor()
    files = ["audio1.wav", "audio2.wav", "audio3.wav"]
    results = await processor.process_batch(files)

asyncio.run(main())
```

## éƒ¨ç½²å®è·µ

### FastAPI æœåŠ¡

```python
"""éŸ³é¢‘å¤„ç† API æœåŠ¡"""

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import FileResponse
from transformers import pipeline
import soundfile as sf
import tempfile
import os

app = FastAPI()

# åˆå§‹åŒ–æ¨¡å‹
asr = pipeline("automatic-speech-recognition", model="openai/whisper-small")
tts = pipeline("text-to-speech", model="suno/bark-small")

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    """è¯­éŸ³è½¬æ–‡æœ¬"""
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name

    try:
        # è½¬å½•
        result = asr(tmp_path)
        return {"text": result["text"]}
    finally:
        os.unlink(tmp_path)

@app.post("/synthesize")
async def synthesize(text: str):
    """æ–‡æœ¬è½¬è¯­éŸ³"""
    # åˆæˆè¯­éŸ³
    speech = tts(text)

    # ä¿å­˜å¹¶è¿”å›
    output_path = tempfile.mktemp(suffix=".wav")
    sf.write(output_path, speech["audio"], samplerate=speech["sampling_rate"])

    return FileResponse(
        output_path,
        media_type="audio/wav",
        filename="output.wav"
    )

@app.get("/health")
async def health():
    return {"status": "healthy"}

# è¿è¡Œ: uvicorn app:app --host 0.0.0.0 --port 8000
```

### Docker éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# é¢„ä¸‹è½½æ¨¡å‹
RUN python -c "from transformers import pipeline; \
    pipeline('automatic-speech-recognition', model='openai/whisper-small'); \
    pipeline('text-to-speech', model='suno/bark-small')"

EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  audio-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### ç›‘æ§ä¸æ—¥å¿—

```python
"""æ·»åŠ ç›‘æ§å’Œæ—¥å¿—"""

import time
import logging
from functools import wraps
from prometheus_client import Counter, Histogram

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Prometheus æŒ‡æ ‡
REQUEST_COUNT = Counter('audio_requests_total', 'Total requests', ['endpoint'])
REQUEST_LATENCY = Histogram('audio_request_latency_seconds', 'Request latency', ['endpoint'])

def monitor(endpoint):
    """ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            REQUEST_COUNT.labels(endpoint=endpoint).inc()
            start_time = time.time()

            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                logger.error(f"Error in {endpoint}: {e}")
                raise
            finally:
                latency = time.time() - start_time
                REQUEST_LATENCY.labels(endpoint=endpoint).observe(latency)
                logger.info(f"{endpoint} completed in {latency:.2f}s")

        return wrapper
    return decorator

# ä½¿ç”¨
@app.post("/transcribe")
@monitor("transcribe")
async def transcribe(file: UploadFile = File(...)):
    # ...
```

## å°ç»“

| åœºæ™¯ | å…³é”®æŠ€æœ¯ | æ³¨æ„äº‹é¡¹ |
|------|----------|----------|
| è¯­éŸ³åŠ©æ‰‹ | ASR + LLM + TTS | ç«¯åˆ°ç«¯å»¶è¿Ÿä¼˜åŒ– |
| ä¼šè®®è½¬å†™ | Whisper + æ—¶é—´æˆ³ | é•¿éŸ³é¢‘åˆ†å—å¤„ç† |
| æœ‰å£°ä¹¦ | TTS + æ–‡æœ¬åˆ†å‰² | è¯­éŸ³è¿è´¯æ€§ |

**éƒ¨ç½²å»ºè®®ï¼š**
1. ä½¿ç”¨ GPU åŠ é€Ÿæ¨ç†
2. æ¨¡å‹é‡åŒ–å‡å°‘æ˜¾å­˜
3. æ‰¹å¤„ç†æé«˜åå
4. æµå¼å¤„ç†é™ä½å»¶è¿Ÿ
5. å¼‚æ­¥ API æé«˜å¹¶å‘

---

*ä¸‹ä¸€èŠ‚ï¼š[TTS æ¨¡å‹å¯¹æ¯”](tts-models.md) - ä¸»æµæ¨¡å‹è¯„æµ‹*
